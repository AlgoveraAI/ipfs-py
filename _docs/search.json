[
  {
    "objectID": "ipfsspec.gateway.html",
    "href": "ipfsspec.gateway.html",
    "title": "ipfsspec gateway",
    "section": "",
    "text": "source\n\nAsyncIPFSGateway\n\n AsyncIPFSGateway (url=None, gateway_type='local')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nGatewayState\n\n GatewayState ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMultiGateway\n\n MultiGateway (gws, max_backoff_rounds=40)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "ipfsspec.asyn.html",
    "href": "ipfsspec.asyn.html",
    "title": "IPFSSpec",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "ipfsspec.asyn.html#changing-gateways",
    "href": "ipfsspec.asyn.html#changing-gateways",
    "title": "IPFSSpec",
    "section": "Changing Gateways",
    "text": "Changing Gateways\nTo learn more about the fsspec api, please see the dock here\nRegister ipfsspec to fsspec,\n\nimport fsspec, os, io, glob, asyncio\nfrom ipfspy.ipfsspec.asyn import AsyncIPFSFileSystem\nfrom fsspec import register_implementation\n\n# register_implementation(IPFSFileSystem.protocol, IPFSFileSystem)\nregister_implementation(AsyncIPFSFileSystem.protocol, AsyncIPFSFileSystem)\n\n# with fsspec.open(\"ipfs://QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx\", \"r\") as f:\n#     print(f.read())\nclass fs:\n    ipfs = fsspec.filesystem(\"ipfs\")\n    file = fsspec.filesystem(\"file\")\n\nChanged to local node\n\n\n\nfs.ipfs.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nfs.ipfs.change_gateway_type = 'local'\n\nChanged to local node"
  },
  {
    "objectID": "ipfsspec.asyn.html#using-local-node",
    "href": "ipfsspec.asyn.html#using-local-node",
    "title": "IPFSSpec",
    "section": "Using Local node",
    "text": "Using Local node\nPut File\n\nfs.ipfs.put(path='output/test.txt', rpath='/test_put_file')\n\n'QmbwFKzLj9m2qwBFYTVrBotf4QujvzTb1GBV9wFcNPMctm'\n\n\nPut Folder\n\nfs.ipfs.put(path='output/fol1/fol2', rpath='/test_put_folder', recursive=True, return_cid=False)\n\n[{'Name': 'fol2/test2.txt',\n  'Hash': 'QmZCFrtagSLhiHKAywF8oWxapXtR6JiJ8GeENASyhLvvyu',\n  'Size': '28'},\n {'Name': 'fol2/test.txt',\n  'Hash': 'QmbwFKzLj9m2qwBFYTVrBotf4QujvzTb1GBV9wFcNPMctm',\n  'Size': '28'},\n {'Name': 'fol2/test3.txt',\n  'Hash': 'QmWT5UmZ4zoXX9GsXpPtEMVP5m1VZ7N6rdxnXHGNkFKqFJ',\n  'Size': '36'},\n {'Name': 'fol2/.ipynb_checkpoints',\n  'Hash': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n  'Size': '4'},\n {'Name': 'fol2',\n  'Hash': 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54',\n  'Size': '312'}]\n\n\nCat File\n\nfs.ipfs.cat('QmbwFKzLj9m2qwBFYTVrBotf4QujvzTb1GBV9wFcNPMctm')\n\nb\"\\n'''\\nFirst file\\n'''\\n\"\n\n\nCat Folder\n\nfs.ipfs.cat('QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54')\n\n{'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test.txt': b\"\\n'''\\nFirst file\\n'''\\n\",\n 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test2.txt': b'```\\nsecond file\\n```\\n',\n 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test3.txt': b\"\\n'''\\nthis is third file\\n'''\\n\"}\n\n\nList a folder\n\nfs.ipfs.ls('QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54')\n\n[{'name': 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/.ipynb_checkpoints',\n  'CID': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n  'type': 'directory',\n  'size': 0},\n {'name': 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test.txt',\n  'CID': 'QmbwFKzLj9m2qwBFYTVrBotf4QujvzTb1GBV9wFcNPMctm',\n  'type': 'file',\n  'size': 20},\n {'name': 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test2.txt',\n  'CID': 'QmZCFrtagSLhiHKAywF8oWxapXtR6JiJ8GeENASyhLvvyu',\n  'type': 'file',\n  'size': 20},\n {'name': 'QmWHpqeDLAMMuLSKDqcMNZRaZGwEKbY5FoU7DMA3pftj54/test3.txt',\n  'CID': 'QmWT5UmZ4zoXX9GsXpPtEMVP5m1VZ7N6rdxnXHGNkFKqFJ',\n  'type': 'file',\n  'size': 28}]\n\n\nGet a file\n\ndef test_get_public_cid(cid='QmWT5UmZ4zoXX9GsXpPtEMVP5m1VZ7N6rdxnXHGNkFKqFJ', \n                        rpath='/test_get_file',\n                        out_lpath = 'output/get_file/', \n                        gateway_type=None):\n    if gateway_type:\n        fs.ipfs.change_gateway_type = 'local'\n    if fs.file.exists(out_lpath):\n        fs.file.rm(out_lpath, recursive=True)\n        \n    fs.ipfs.rm(rpath, recursive=True)\n    print('Before: ', [p.lstrip(os.getcwd()) for p in fs.file.glob(f'{out_lpath}/*')])\n    \n    fs.ipfs.get(rpath=cid, lpath=out_lpath, recursive=True, return_cid=False)\n    print('After: ', [p.lstrip(os.getcwd()) for p in fs.file.glob(f'{out_lpath}/*')])\n    \n    fs.ipfs.rm(rpath, recursive=True)\n\ntest_get_public_cid()\n\nBefore:  []\nAfter:  ['output/get_file/QmWT5UmZ4zoXX9GsXpPtEMVP5m1VZ7N6rdxnXHGNkFKqFJ']"
  },
  {
    "objectID": "pinataapi.html",
    "href": "pinataapi.html",
    "title": "Pinata",
    "section": "",
    "text": "with open(\"creds.json\") as f:\n    creds = json.loads(f.read())[\"Pinata\"][\"JWT\"]\n\n\nsource\n\ngenerate_apikey\n\n generate_apikey (cred:str, key_name:str, pinlist:bool=False,\n                  userPinnedDataTotal:bool=False, hashMetadata:bool=True,\n                  hashPinPolicy:bool=False, pinByHash:bool=True,\n                  pinFileToIPFS:bool=True, pinJSONToIPFS:bool=True,\n                  pinJobs:bool=True, unpin:bool=True,\n                  userPinPolicy:bool=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT\n\n\nkey_name\nstr\n\nKey name\n\n\npinlist\nbool\nFalse\nlist pins\n\n\nuserPinnedDataTotal\nbool\nFalse\ntotal data stored\n\n\nhashMetadata\nbool\nTrue\nmetadata\n\n\nhashPinPolicy\nbool\nFalse\npolicy\n\n\npinByHash\nbool\nTrue\npin cid\n\n\npinFileToIPFS\nbool\nTrue\nupload file to IPFS\n\n\npinJSONToIPFS\nbool\nTrue\nupload json to IPFS\n\n\npinJobs\nbool\nTrue\nsee pin jobs\n\n\nunpin\nbool\nTrue\nunpin ipfs cid\n\n\nuserPinPolicy\nbool\nTrue\nestablish pin policy\n\n\n\nGenerate API Key\n\nresponse = generate_apikey(creds,\"Test\")\nresponse.json()\n\n{'pinata_api_key': '443e2dadf1e6a47c2754',\n 'pinata_api_secret': '0f84d67fe64136d4d9ddbfb1235a8ab1fd490f12c1f6fcc532b758062695a1dc',\n 'JWT': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiJkMjhmMjBjZi1kZmUwLTRjOGYtOTI3Mi0yNmI5YzJkOGEzY2QiLCJlbWFpbCI6InZpbnRhZ2Vnb2xkMTIzQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJwaW5fcG9saWN5Ijp7InJlZ2lvbnMiOlt7ImlkIjoiTllDMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfV0sInZlcnNpb24iOjF9LCJtZmFfZW5hYmxlZCI6ZmFsc2UsInN0YXR1cyI6IkFDVElWRSJ9LCJhdXRoZW50aWNhdGlvblR5cGUiOiJzY29wZWRLZXkiLCJzY29wZWRLZXlLZXkiOiI0NDNlMmRhZGYxZTZhNDdjMjc1NCIsInNjb3BlZEtleVNlY3JldCI6IjBmODRkNjdmZTY0MTM2ZDRkOWRkYmZiMTIzNWE4YWIxZmQ0OTBmMTJjMWY2ZmNjNTMyYjc1ODA2MjY5NWExZGMiLCJpYXQiOjE2NTg0OTk3NDF9.Rd4F1zBE8wTPIwi4mD1x3pdEnfEM2Ik7zjiw0vQ7AC8'}\n\n\n\nsource\n\n\nlist_apikeys\n\n list_apikeys (cred:str)\n\nList API Keys\n\nnewest_apikey = list_apikeys(creds).json()[\"keys\"][0];newest_apikey\n\n{'id': '1ff9d58d-d7fa-4598-9863-c53a806faff6',\n 'name': 'Test',\n 'key': '443e2dadf1e6a47c2754',\n 'secret': 'ed7a3bdc89fb353b6c4f4155a9adb7f5:d15adb31f816663e2612ff5f89a5497abc30a5100ab83575247ba67cc9d49a7d61293d77c94049a8d83a82d8f02423357f311b26204bb57e7be9181d324e6424',\n 'max_uses': None,\n 'uses': 0,\n 'user_id': 'd28f20cf-dfe0-4c8f-9272-26b9c2d8a3cd',\n 'scopes': {'endpoints': {'data': {'pinList': False,\n    'userPinnedDataTotal': False},\n   'pinning': {'unpin': True,\n    'pinJobs': True,\n    'pinByHash': True,\n    'hashMetadata': True,\n    'hashPinPolicy': False,\n    'pinFileToIPFS': True,\n    'pinJSONToIPFS': True,\n    'userPinPolicy': True}}},\n 'revoked': False,\n 'createdAt': '2022-07-22T14:22:21.191Z',\n 'updatedAt': '2022-07-22T14:22:21.191Z'}\n\n\n\nsource\n\n\nrevoke_apikey\n\n revoke_apikey (cred:str, revoke_apikey:str)\n\nRevoke API Key\n\nrevoke_apikey(creds,response.json()[\"JWT\"])\n\n<Response [200]>\n\n\n\nsource\n\n\nupload_file\n\n upload_file (cred:str, name:str, fpaths:list, metadata:dict,\n              cid_version:str='1', directory:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\nname\nstr\n\nfilename\n\n\nfpaths\nlist\n\nfilepaths\n\n\nmetadata\ndict\n\nmetadata\n\n\ncid_version\nstr\n1\nIPFS cid\n\n\ndirectory\nbool\nFalse\nupload directory\n\n\n\nUpload File to IPFS with metadata\n\nmetadata = {\"type\":\"AdultData\"}\n\nupload_file(creds,\"adult_data.csv\",\"output/adult_data.csv\",metadata,cid_version=1,directory=False).text\n\n'{\"IpfsHash\":\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"PinSize\":3975284,\"Timestamp\":\"2022-06-17T22:43:08.303Z\",\"isDuplicate\":true}'\n\n\n\nsource\n\n\nupload_jsonfile\n\n upload_jsonfile (cred:str, name:str, fpaths:list, metadata:dict,\n                  cid_version:str, directory:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\nname\nstr\n\nfilename\n\n\nfpaths\nlist\n\nfilepaths\n\n\nmetadata\ndict\n\nmetadata\n\n\ncid_version\nstr\n\nIPFS cid\n\n\ndirectory\nbool\nFalse\nupload directory\n\n\n\nThis endpoint is optimized for JSON files\n\nmetadata = {\"name\":\"Vote\"}\n\nupload_jsonfile(creds,\"ens_airdrop_Nov8th2021.json\",\"output/ens_airdrop_Nov8th2021.json\",metadata,cid_version=1,directory=False).text\n\n'{\"IpfsHash\":\"bafkreid56by3qspgyxvppq3ggj2x4tec3akpnlffbhangjzzfc3lfh66yq\",\"PinSize\":45,\"Timestamp\":\"2022-07-22T14:29:51.674Z\",\"isDuplicate\":true}'\n\n\n\nsource\n\n\npin\n\n pin (cred:str, cid:str, fn=None, pinataMetadata=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT key\n\n\ncid\nstr\n\nIPFS cid\n\n\nfn\nNoneType\nNone\nName of file\n\n\npinataMetadata\nNoneType\nNone\nAdd keys and values associated with IPFS CID\n\n\n\nPin files\n\nmetadata = {\"dApp\":\"Ethereum Name Service\",\n            \"token\":\"ENS\"\n           }\n\npin(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",fn=\"ens_airdrop\",pinataMetadata=metadata)\n\n{\"id\":\"8cba5496-3cd4-4edb-92d7-023c0ce9c145\",\"ipfsHash\":\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",\"status\":\"prechecking\",\"name\":\"ens_airdrop\"}\n\n\n\nsource\n\n\nunpin\n\n unpin (cred:str, cid:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\ncred\nstr\nJWT Key\n\n\ncid\nstr\nIPFS CID\n\n\n\nUnpin IPFS CID\n\nunpin(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\")\n\n<Response [200]>\n\n\nIf IPFS CID isn’t pinned. A 500 error message will be returned\n\nunpin(creds,\"QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V\")\n\n<Response [500]>\n\n\n\nsource\n\n\nedit_metadata\n\n edit_metadata (cred:str, cid:str, name:str, metadata=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\ncid\nstr\n\nIPFS CID\n\n\nname\nstr\n\nfilename\n\n\nmetadata\nNoneType\nNone\nAdd keys and values associated with IPFS CID\n\n\n\nEdit metadata of already pinned IPFS CID\n\nmetadata = {\"type\":\"Test\"}\n\nedit_metadata(creds,\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"name\",metadata)\n\n<Response [200]>\n\n\nIf IPFS CID is not actively pinned. A 500 error message will be returned\n\nmetadata = {\"type\":\"Test\"}\n\nedit_metadata(creds,\"QmUUSHH2ycqciruPaRcptpUQDDeRiMV3G9PxA6KUupXTJV\",\"name\",metadata)\n\n<Response [500]>\n\n\n\nsource\n\n\nget_pinned_jobs\n\n get_pinned_jobs (cred:str, params=None)\n\n‘sort’ - Sort the results by the date added to the pinning queue (see value options below) ‘ASC’ - Sort by ascending dates ‘DESC’ - Sort by descending dates ‘status’ - Filter by the status of the job in the pinning queue (see potential statuses below) ‘prechecking’ - Pinata is running preliminary validations on your pin request. ‘searching’ - Pinata is actively searching for your content on the IPFS network. This may take some time if your content is isolated. ‘retrieving’ - Pinata has located your content and is now in the process of retrieving it. ‘expired’ - Pinata wasn’t able to find your content after a day of searching the IPFS network. Please make sure your content is hosted on the IPFS network before trying to pin again. ‘over_free_limit’ - Pinning this object would put you over the free tier limit. Please add a credit card to continue pinning content. ‘over_max_size’ - This object is too large of an item to pin. If you’re seeing this, please contact us for a more custom solution. ‘invalid_object’ - The object you’re attempting to pin isn’t readable by IPFS nodes. Please contact us if you receive this, as we’d like to better understand what you’re attempting to pin. ‘bad_host_node’ - You provided a host node that was either invalid or unreachable. Please make sure all provided host nodes are online and reachable. ‘ipfs_pin_hash’ - Retrieve the record for a specific IPFS hash ‘limit’ - Limit the amount of results returned per page of results (default is 5, and max is 1000) ‘offset’ - Provide the record offset for records being returned. This is how you retrieve records on additional pages (default is 0)\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nfiltering pinned jobs\n\n\n\nRetrieve pinned jobs. If there are pending pin jobs, they will be returned in json format.\n\nget_pinned_jobs(creds).json()\n\n{'count': 0, 'rows': []}\n\n\n\nsource\n\n\nget_pinned_files\n\n get_pinned_files (cred:str, params=None)\n\nQuery Parameters = params\nhashContains: (string) - Filter on alphanumeric characters inside of pin hashes. Hashes which do not include the characters passed in will not be returned. pinStart: (must be in ISO_8601 format) - Exclude pin records that were pinned before the passed in ‘pinStart’ datetime. pinEnd: (must be in ISO_8601 format) - Exclude pin records that were pinned after the passed in ‘pinEnd’ datetime. unpinStart: (must be in ISO_8601 format) - Exclude pin records that were unpinned before the passed in ‘unpinStart’ datetime. unpinEnd: (must be in ISO_8601 format) - Exclude pin records that were unpinned after the passed in ‘unpinEnd’ datetime. pinSizeMin: (integer) - The minimum byte size that pin record you’re looking for can have pinSizeMax: (integer) - The maximum byte size that pin record you’re looking for can have status: (string) - * Pass in ‘all’ for both pinned and unpinned records * Pass in ‘pinned’ for just pinned records (hashes that are currently pinned) * Pass in ‘unpinned’ for just unpinned records (previous hashes that are no longer being pinned on pinata) pageLimit: (integer) - This sets the amount of records that will be returned per API response. (Max 1000) pageOffset: (integer) - This tells the API how far to offset the record responses. For example, if there’s 30 records that match your query, and you passed in a pageLimit of 10, providing a pageOffset of 10 would return records 11-20.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nFilter returned pinned files\n\n\n\nRetrieve pinned files and use filtering arguments such as hashContains and status to filter by IPFS CID and get only the file that is pinned. Pinata keeps a log of all the times an IPFS CID is pinned and unpinned. Therefore, without the status filter multiple records would be returned.\n\nget_pinned_files(creds,params={\"hashContains\":\"bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie\",\"status\":\"pinned\"}).json()\n\n{'count': 1,\n 'rows': [{'id': 'a1a93991-99bc-4c12-9553-2e689046cf65',\n   'ipfs_pin_hash': 'bafybeibcsnxt2sqzsvfg4lzg3iuxkfit37l3rz5gqmdxwsgec56qalmxie',\n   'size': 3975284,\n   'user_id': 'd28f20cf-dfe0-4c8f-9272-26b9c2d8a3cd',\n   'date_pinned': '2022-06-17T22:43:08.303Z',\n   'date_unpinned': None,\n   'metadata': {'name': 'name',\n    'keyvalues': {'Type': 'Test',\n     'type': 'Test',\n     'company': 'Pinata',\n     'filetype': 'json'}},\n   'regions': [{'regionId': 'NYC1',\n     'currentReplicationCount': 1,\n     'desiredReplicationCount': 1}]}]}\n\n\n\nsource\n\n\nget_datausage\n\n get_datausage (cred:str, params=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncred\nstr\n\nJWT Key\n\n\nparams\nNoneType\nNone\nFilter returned data usage statistics\n\n\n\nRetrieve data usage stats. The stats are in bytes.\n\nget_datausage(creds).json()\n\n{'pin_count': -223,\n 'pin_size_total': 349265187,\n 'pin_size_with_replications_total': 349265187}"
  },
  {
    "objectID": "ipfshttpapi.html",
    "href": "ipfshttpapi.html",
    "title": "IPFS HTTP API",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "ipfshttpapi.html#choosing-your-gateway",
    "href": "ipfshttpapi.html#choosing-your-gateway",
    "title": "IPFS HTTP API",
    "section": "Choosing your gateway",
    "text": "Choosing your gateway\nBy default, the IPFSApi uses the local node.\n\napi = IPFSApi()\n\nChanged to local node\n\n\nTo change to infura or public use the change_gateway_type method\nInfura needs authorization, you can use the make_infura_auth to make the auth in required type. And when using the chage_gateway_type pass auth as the second variable in the tuple. For public and local, you can pass None as the second variable.\n\napi.change_gateway_type = 'infura', auth\n\n\napi.change_gateway_type = 'public', None\n\n\napi.change_gateway_type = 'local', None"
  },
  {
    "objectID": "ipfshttpapi.html#unixfs",
    "href": "ipfshttpapi.html#unixfs",
    "title": "IPFS HTTP API",
    "section": "UnixFS",
    "text": "UnixFS\n\nadd_items\n\nonly available on local and infura nodes\n\n\nsource\n\n\nIPFSApi.add_items\n\n IPFSApi.add_items (filepath:Union[str,List[str]], directory:bool=False,\n                    wrap_with_directory:bool=False,\n                    chunker:str='size-262144', pin:bool=True,\n                    hash_:str='sha2-256', progress:str='true',\n                    silent:str='false', cid_version:int=0, **kwargs)\n\nadd file/directory to ipfs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilepath\ntyping.Union[str, typing.List[str]]\n\nPath to the file/directory to be added to IPFS\n\n\ndirectory\nbool\nFalse\nIs filepath a directory\n\n\nwrap_with_directory\nbool\nFalse\nTrue if path is a directory\n\n\nchunker\nstr\nsize-262144\nChunking algorithm, size-[bytes], rabin-[min]-[avg]-[max] or buzhash\n\n\npin\nbool\nTrue\nPin this object when adding\n\n\nhash_\nstr\nsha2-256\nHash function to use. Implies CIDv1 if not sha2-256\n\n\nprogress\nstr\ntrue\nStream progress data\n\n\nsilent\nstr\nfalse\nWrite no output\n\n\ncid_version\nint\n0\nCID version\n\n\nkwargs\n\n\n\n\n\n\n\nUsing local IPFS Node\n\nImportant: This requires a local IPFS Node to be run. Set up your local node using info here.\n\n\nNote: Full list of available params on add function can be found here\n\nUpload a file\n\nresponse, jsonobject = api.add_items(filepath=\"../README.md\"); jsonobject\n\nUpload multiple files\n\nresponse, jsonobject = api.add_items(filepath=[\"../README.md\", 'output/test.txt']); jsonobject\n\nYou can view the added file on ipfs.io\nTo view any CID on ipfs.io https://ipfs.io/ipfs/CID\nUpload multiple file wrapped in a directory\n\nresponse, jsonobject = api.add_items(filepath=[\"../README.md\", 'output/test.txt'], wrap_with_directory='true'); jsonobject\n\nUpload a directory\n\nresponse, jsonobject = api.add_items(filepath='output', directory=True); jsonobject[-4:]\n\n\n\nUsing infura.io gateway\nThis works without a local IPFS node running but requires registration. You can register here\n\napi.change_gateway_type = 'infura', auth\n\n\nresponse, jsonobject = api.add_items(\"../README.md\"); jsonobject\n\n\nresponse, jsonobject = api.add_items(filepath='output', directory=True); jsonobject[-4:]\n\nYou can view the added file on infura.io\nTo view any CID on infura https://ipfs.infura.io/ipfs/CID\n\n\n\nls_items\n\nonly available on local and public nodes. ls not implemented on infura\n\n\nsource\n\n\nIPFSApi.ls_items\n\n IPFSApi.ls_items (cid:str, resolve_type:bool=True, size:bool=True,\n                   **kwargs)\n\nList directory contents for Unix filesystem objects\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nThe path to the IPFS object(s) to list links from\n\n\nresolve_type\nbool\nTrue\nResolve linked objects to find out their types\n\n\nsize\nbool\nTrue\nResolve linked objects to find out their file size\n\n\nkwargs\n\n\n\n\n\n\n\napi.change_gateway_type = 'local'\n\n\nresponse, content = api.ls_items('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n\napi.change_gateway_type = 'public'\n\n\nresponse, content = api.ls_items('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n\n\ncat_items\n\nsource\n\n\nIPFSApi.cat_items\n\n IPFSApi.cat_items (cid:str, **kwargs)\n\nShow IPFS object data\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS object(s) to be output\n\n\nkwargs\n\n\n\n\n\nWhen given a file CID\n\napi.change_gateway_type = 'local'\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\nWhen given a directory CID (this will throw an error)\n\nresponse, content = api.cat_items(cid='QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\n\napi.change_gateway_type = 'public'\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n\napi.change_gateway_type = 'infura'\n\n\nresponse, content = api.cat_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n\n\nget_items\n\nresponse, content = api.get_items(cid='QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\n\n\nHow to download a directory from IPFS\n\nonly available on local and public\n\n\nsource\n\n\nDownloadDir\n\n DownloadDir (gateway_type:str, root_cid:str, output_fol:str)\n\nDownload a IPFS directory to your local disk\n\n\n\n\nType\nDetails\n\n\n\n\ngateway_type\nstr\nGateway to use - works on local and public\n\n\nroot_cid\nstr\nRoot CID of the directory\n\n\noutput_fol\nstr\nPath to save in your local disk\n\n\n\n\ndownload = DownloadDir('local', 'QmWJvSt7QGY2yhr9mrfAm76vRvxXA5ygB4zd2SmfD43vtX', 'output2')\n\n\ndownload.download()\n\n\ndownload.full_structure"
  },
  {
    "objectID": "ipfshttpapi.html#dags",
    "href": "ipfshttpapi.html#dags",
    "title": "IPFS HTTP API",
    "section": "DAGs",
    "text": "DAGs\n\nsource\n\nIPFSApi.dag_get\n\n IPFSApi.dag_get (cid:str, output_codec:str='dag-json')\n\nGet a DAG node from IPFS.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nThe path to the IPFS DAG node\n\n\noutput_codec\nstr\ndag-json\n\n\n\n\n\nsource\n\n\nIPFSApi.dag_export\n\n IPFSApi.dag_export (cid:str, **kwargs)\n\nStreams the selected DAG as a .car stream on stdout.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS DAG node\n\n\nkwargs\n\n\n\n\n\n\nsource\n\n\nIPFSApi.dag_stat\n\n IPFSApi.dag_stat (cid:str, **kwargs)\n\nGets stats for a DAG.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe path to the IPFS DAG node\n\n\nkwargs\n\n\n\n\n\n\n\nMaking use of DAGs\nLet’s take a look at the DAG structure of the folder we uploaded earlier\n\nresponse, content = api.dag_get('QmeyTiqrD6oo4eQXbAt7hZn3ASzpcP3Kvb1rc9qznEowEw'); content\n\ndag_get method returns a dict with data and links. data is not exciting for a directory. Under the links, we can see files/directories that are in the uploaded directory.\nLet’s see the number of files/directories in the links\n\nlen(content['Links'])\n\nLet’s take a look at the first link\n\ncontent['Links'][0]"
  },
  {
    "objectID": "ipfshttpapi.html#local-pinning",
    "href": "ipfshttpapi.html#local-pinning",
    "title": "IPFS HTTP API",
    "section": "Local Pinning",
    "text": "Local Pinning\n\nPin an IPFS object locally\nMore info on this can be found here\n\nsource\n\n\nIPFSApi.pin_add\n\n IPFSApi.pin_add (cid:str, recursive:str='true')\n\nPin objects to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to IPFS object(s) to be pinned\n\n\nrecursive\nstr\ntrue\nRecursively pin the object linked to by the specified object(s)\n\n\n\n\nsource\n\n\nIPFSApi.pin_ls\n\n IPFSApi.pin_ls (type_:str='all', **kwargs)\n\nList objects pinned to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntype_\nstr\nall\nThe type of pinned keys to list. Can be “direct”, “indirect”, “recursive”, or “all”\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nIPFSApi.pin_rm\n\n IPFSApi.pin_rm (cid:str, recursive:str='true', **kwargs)\n\nList objects pinned to local storage.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to object(s) to be unpinned\n\n\nrecursive\nstr\ntrue\nRecursively unpin the object linked to by the specified object(s)\n\n\nkwargs\n\n\n\n\n\n\n\nresponse, content = api.pin_add('QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); content\n\nLet’s list all the items pinned locally.\n\nresponse, content = api.pin_ls(); len(content[0]['Keys'])\n\n\nlist(content[0]['Keys'].keys())[:10]\n\nThe item we added above is also part of it.\n\ncontent[0]['Keys']['QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B']\n\nWhat does the Type : recursive means? - Direct pins - Single block and no others in relation to it. - Recursive pins - Given block and all of its children. - Indirect pins - the result of a given block’s parent being pinned recursively.\n\nresponse, content = api.pin_rm('QmUfwG4P6EA5xbD3De5bS7XKcBion8ReQj7m9ZjxaPvq3B'); response.status_code"
  },
  {
    "objectID": "ipfshttpapi.html#remote-pinning-service",
    "href": "ipfshttpapi.html#remote-pinning-service",
    "title": "IPFS HTTP API",
    "section": "Remote Pinning Service",
    "text": "Remote Pinning Service\n\nsource\n\nIPFSApi.rspin_add\n\n IPFSApi.rspin_add (service_name:str, service_edpt:str, service_key:str)\n\nPin object to remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice_name\nstr\nName of the remote pinning service to use\n\n\nservice_edpt\nstr\nService endpoint\n\n\nservice_key\nstr\nService key\n\n\n\n\nsource\n\n\nIPFSApi.rspin_ls\n\n IPFSApi.rspin_ls (**kwargs)\n\nList remote pinning services.\n\nsource\n\n\nIPFSApi.rspin_rm\n\n IPFSApi.rspin_rm (service_name:str)\n\nRemove remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice_name\nstr\nName of pinning service to remove\n\n\n\n\n\nWork with remote pinning service\nMore info on this can be found here and here\nAdding your Pinata to your IPFS node 1. Create an account with Pinata 2. use rspin_add method to add the service\n\nos.environ[\"pinata_api_secret\"] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiJhZDkxMTQ3Yy0yMmE5LTQ1MjgtODk2OS05ZTdjMjAwNzE2ZjAiLCJlbWFpbCI6Im1hcnNoYXRoQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJwaW5fcG9saWN5Ijp7InJlZ2lvbnMiOlt7ImlkIjoiRlJBMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfV0sInZlcnNpb24iOjF9LCJtZmFfZW5hYmxlZCI6ZmFsc2V9LCJhdXRoZW50aWNhdGlvblR5cGUiOiJzY29wZWRLZXkiLCJzY29wZWRLZXlLZXkiOiIxMjc4NDQ5OGY3YzI0ZjczNDBjMiIsInNjb3BlZEtleVNlY3JldCI6ImQyYmNlZjFlODVlZjgyOTcwNGE0ZTk4NGRmYjc3ZWE0MDhlY2NkODk1MjMxMzI2YjVlMmZlZDQxMTBhZmQyMGEiLCJpYXQiOjE2NTM5Njk0MDN9.Fa055BjXNmERyl_ZAA9NJscdt0HqbiX0ByY8pU5uLNY'\n\n\nNote: For pinata the service_key is also called JWT\n\n\n\n\nimage.png\n\n\n\nres, content = api.rspin_add('pinata', 'https://api.pinata.cloud/psa', os.environ['pinata_api_secret']);res.status_code\n\nLet’s list the pinned services to see if we successfully added pinata\nPinata has been added\n\nresponse, content = api.rspin_ls(); content"
  },
  {
    "objectID": "ipfshttpapi.html#remote-pinning",
    "href": "ipfshttpapi.html#remote-pinning",
    "title": "IPFS HTTP API",
    "section": "Remote Pinning",
    "text": "Remote Pinning\n\nsource\n\nIPFSApi.rpin_add\n\n IPFSApi.rpin_add (cid:str, service:str, background:str='false', **kwargs)\n\nPin object to remote pinning service.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nPath to IPFS object(s) to be pinned\n\n\nservice\nstr\n\nName of the remote pinning service to use\n\n\nbackground\nstr\nfalse\nAdd to the queue on the remote service and return immediately (does not wait for pinned status)\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nIPFSApi.rpin_ls\n\n IPFSApi.rpin_ls (service:str, **kwargs)\n\nList objects pinned to remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice\nstr\nName of the remote pinning service to use\n\n\nkwargs\n\n\n\n\n\n\nsource\n\n\nIPFSApi.rpin_rm\n\n IPFSApi.rpin_rm (service:str, **kwargs)\n\nRemove pins from remote pinning service.\n\n\n\n\nType\nDetails\n\n\n\n\nservice\nstr\nName of the remote pinning service to use\n\n\nkwargs\n\n\n\n\n\nLet’s add a new file to IPFS and pin it to pinata.\n\nresponse, content = api.add_items(filepath='output/test.txt'); content\n\n\nresponse, content = api.rpin_add('QmTT8vwdbnP9Ls8bSY1LMyW4a8bEwTYZa5izEoJMBtTPfb', 'pinata'); response.status_code\n\nAs can ber seen, we have successfully added the file to pinata\n\nresponse, content = api.rpin_ls('pinata'); content"
  },
  {
    "objectID": "ipfshttpapi.html#block",
    "href": "ipfshttpapi.html#block",
    "title": "IPFS HTTP API",
    "section": "Block",
    "text": "Block\n\nsource\n\nIPFSApi.block_get\n\n IPFSApi.block_get (cid:str)\n\nGet a raw IPFS block.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nThe base58 multihash of an existing block to get\n\n\n\n\nsource\n\n\nIPFSApi.block_put\n\n IPFSApi.block_put (filepath:str, mhtype:str='sha2-256', mhlen:int=-1,\n                    pin:str=False, **kwargs)\n\nStore input as an IPFS block.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilepath\nstr\n\nPath to file\n\n\nmhtype\nstr\nsha2-256\nmultihash hash function.\n\n\nmhlen\nint\n-1\nMultihash hash length\n\n\npin\nstr\nFalse\npin added blocks recursively\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nIPFSApi.block_rm\n\n IPFSApi.block_rm (cid:str, force:str='false', quiet:str='false')\n\nRemove IPFS block(s).\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncid\nstr\n\nBash58 encoded multihash of block(s) to remove\n\n\nforce\nstr\nfalse\nIgnore nonexistent blocks.\n\n\nquiet\nstr\nfalse\nWrite minimal output.\n\n\n\n\nsource\n\n\nIPFSApi.block_stat\n\n IPFSApi.block_stat (cid:str)\n\nPrint information of a raw IPFS block.\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nBash58 encoded multihash of block(s) to remove\n\n\n\n\n\nWork with blocks\nTo explore the blocks modeule, let’s add a large file\n\nresponse, content = api.add_items('output/adult_data.csv'); content\n\nAs you can see from below, adding it results in blocks and we are given the root CID\nWe can take a look at the CIDs of different block like below\n\nresponse, content = api.ls_items('QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V');content\n\n\nblocks = content[0][\"Objects\"][0][\"Links\"]\n\n\nprint(f'Number of blocks: {len(blocks)}')\n\nHere, we can see the three blocks of the total 16 blocks.\n\nblocks[:3]\n\nLet’s read whats in the first and second block\n\nresponse, content = api.cat_items(blocks[0]['Hash'])\n\n\nprint(response.text[:100]); len(response.text)\n\n\nresponse, content = api.cat_items(blocks[1]['Hash'])\n\n\nprint(response.text[:100]); len(response.text)\n\nWe can also read using the cat method under the blocks module in which case if block is an intermediate block it will only print the raw binary data.\n\nresponse, content = api.block_get(blocks[0]['Hash'])\n\n\nresponse.text[:300]\n\n\nAdd a block\n\nresponse, content = api.block_put('../README.md'); content\n\n\n\nGet stat on a block\n\nresponse, content = api.block_stat('bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa'); content\n\n\n\nRemove a block\n\nresponse, content = api.block_rm('bafkreicb2n4nhac6nwsviqe2ik4pt22ukewf7zfz2j2aof2au3a2qgi3oa'); response.status_code"
  },
  {
    "objectID": "ipfshttpapi.html#mutable-file-system-files",
    "href": "ipfshttpapi.html#mutable-file-system-files",
    "title": "IPFS HTTP API",
    "section": "Mutable File System (files)",
    "text": "Mutable File System (files)\n\nsource\n\nIPFSApi.mfs_chcid\n\n IPFSApi.mfs_chcid (path:str='/', cid_version:int=0, **kwargs)\n\nChange the CID version or hash function of the root node of a given path.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to change\n\n\ncid_version\nint\n0\nCid version to use\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_cp\n\n IPFSApi.mfs_cp (source_path:str, dest_path:str, **kwargs)\n\nAdd references to IPFS files and directories in MFS (or copy within MFS).\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\nSource IPFS or MFS path to copy\n\n\ndest_path\nstr\nDestination within MFS\n\n\nkwargs\n\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_flush\n\n IPFSApi.mfs_flush (path:str='/')\n\nFlush a given path’s data to disk\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to flush\n\n\n\n\nsource\n\n\nIPFSApi.mfs_ls\n\n IPFSApi.mfs_ls (path:str='/', **kwargs)\n\nList directories in the local mutable namespace.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n/\nPath to show listing for\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_mkdir\n\n IPFSApi.mfs_mkdir (path:str, **kwargs)\n\nMake directories.\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\nPath to dir to make\n\n\nkwargs\n\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_mv\n\n IPFSApi.mfs_mv (source_path:str, dest_path:str)\n\nMove files.\n\n\n\n\nType\nDetails\n\n\n\n\nsource_path\nstr\nSource file to move\n\n\ndest_path\nstr\nDestination path for file to be moved to\n\n\n\n\nsource\n\n\nIPFSApi.mfs_read\n\n IPFSApi.mfs_read (path, **kwargs)\n\nRead a file in a given MFS.\n\n\n\n\nDetails\n\n\n\n\npath\nPath to file to be read\n\n\nkwargs\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_rm\n\n IPFSApi.mfs_rm (path, **kwargs)\n\nRead a file in a given MFS.\n\n\n\n\nDetails\n\n\n\n\npath\nFile to remove\n\n\nkwargs\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_stat\n\n IPFSApi.mfs_stat (path, **kwargs)\n\nDisplay file status.\n\n\n\n\nDetails\n\n\n\n\npath\nPath to node to stat\n\n\nkwargs\n\n\n\n\n\nsource\n\n\nIPFSApi.mfs_write\n\n IPFSApi.mfs_write (path, filepath, create=True, **kwargs)\n\nDisplay file status.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\n\n\nPath to write to\n\n\nfilepath\n\n\nFile to add\n\n\ncreate\nbool\nTrue\nCreate the file if it does not exist\n\n\nkwargs\n\n\n\n\n\n\n\n\nWorking with MFS\nMaking a directory\n\nresponse, content = api.mfs_mkdir('/test'); response.status_code"
  },
  {
    "objectID": "tutorial.ipfsspec.html",
    "href": "tutorial.ipfsspec.html",
    "title": "Tutorial - Using ipfspy.ipfspec to interact with IPFS",
    "section": "",
    "text": "import fsspec\nimport os\nfrom ipfspy.ipfsspec.asyn import AsyncIPFSFileSystem\nfrom fsspec import register_implementation\nimport asyncio\nimport io"
  },
  {
    "objectID": "tutorial.ipfsspec.html#put-file",
    "href": "tutorial.ipfsspec.html#put-file",
    "title": "Tutorial - Using ipfspy.ipfspec to interact with IPFS",
    "section": "Put File",
    "text": "Put File\n\n# put in a file and store it into test folder 'fam'\nfs.ipfs.rm('/test_put_file')\nput_file_res = fs.ipfs.put(path='output/test.txt', rpath='/test_put_file')\nfs.ipfs.ls('/test_put_file')\n\n[{'name': '/test_put_file/test.txt',\n  'CID': 'QmbwFKzLj9m2qwBFYTVrBotf4QujvzTb1GBV9wFcNPMctm',\n  'type': 'file',\n  'size': 20}]"
  },
  {
    "objectID": "tutorial.ipfsspec.html#put-folder",
    "href": "tutorial.ipfsspec.html#put-folder",
    "title": "Tutorial - Using ipfspy.ipfspec to interact with IPFS",
    "section": "Put Folder",
    "text": "Put Folder\n\n# put a directory and store it into test folder\nfs.ipfs.put(path='output/fol1/fol2', rpath='/test_put_folder' ,recursive=True, return_cid=False)\nfs.ipfs.ls('/')\n\n[{'name': '/test_dataset',\n  'CID': 'QmXuV4QHUPyonn82iotczv9pWQndcUt7PFMsJWtTN2Cb1J',\n  'type': 'directory',\n  'size': 0},\n {'name': '/test_get_folder',\n  'CID': 'QmRWz2opj9To63PX4Jt9uYs39FppGrfbui1BkQjbjmxRiQ',\n  'type': 'directory',\n  'size': 0},\n {'name': '/test_model',\n  'CID': 'QmZCsKFtvCWr3opi5mkS8Vm288QCD6sxEnUwC2L91knc2r',\n  'type': 'directory',\n  'size': 0},\n {'name': '/test_put_file',\n  'CID': 'QmcSHBc2XV69PXy6nLQFrHyuNH25JL39VhUYQakiYGn86y',\n  'type': 'directory',\n  'size': 0},\n {'name': '/test_put_folder',\n  'CID': 'QmRWz2opj9To63PX4Jt9uYs39FppGrfbui1BkQjbjmxRiQ',\n  'type': 'directory',\n  'size': 0},\n {'name': '/test_rm_file',\n  'CID': 'QmUNLLsPACCz1vLxQVkXqqLX5R1X345qqfHbsf67hvA3Nn',\n  'type': 'directory',\n  'size': 0}]"
  },
  {
    "objectID": "tutorial.ipfsspec.html#get-remote-file",
    "href": "tutorial.ipfsspec.html#get-remote-file",
    "title": "Tutorial - Using ipfspy.ipfspec to interact with IPFS",
    "section": "Get Remote File",
    "text": "Get Remote File\n\nif fs.file.exists('output/get/test.txt'):\n    fs.file.rm('output/get/test.txt')\n    \nfs.ipfs.put(path='output/test.txt', rpath='/test_get_file' ,recursive=True, return_cid=False)\nprint('Before: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/get/*.txt')])\n\nfs.ipfs.get(rpath='/test_get_file/test.txt', lpath='output/get/test.txt',recursive=True, return_cid=False)\nprint('After: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/get/*.txt')])\nfs.ipfs.rm('/test_get_file')\n\nBefore:  []\nAfter:  ['output/get/test.txt']\n\n\n['/test_get_file']"
  },
  {
    "objectID": "tutorial.ipfsspec.html#get-remote-folder",
    "href": "tutorial.ipfsspec.html#get-remote-folder",
    "title": "Tutorial - Using ipfspy.ipfspec to interact with IPFS",
    "section": "Get Remote Folder",
    "text": "Get Remote Folder\n\nif fs.file.exists('output/get'):\n    fs.file.rm('output/get', recursive=True)\n    \nfs.ipfs.put(lpath='output/fol1/fol2', rpath='/test_get_folder')\nprint('Before: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/get/*.txt')])\nfs.ipfs.get(rpath='/test_get_folder', lpath='output/get',recursive=True, return_cid=False)\nprint('After: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/get/*.txt')])\n\nBefore:  []\nAfter:  ['output/get/test.txt', 'output/get/test2.txt', 'output/get/test3.txt']"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "# #export\n# GATEWAYS_API_READ = [\n#     \"https://ipfs.io/api/v0\",\n#     \"https://gateway.pinata.cloud/api/v0\",\n#     \"https://cloudflare-ipfs.com/api/v0\",\n#     \"https://dweb.link/api/v0\",\n#     \"https://ipfs.eth.aragon.network/api/v0\",\n#     \"https://permaweb.eu.org/api/v0\",\n#     \"https://nftstorage.link/api/v0\",\n#     \"https://ipfs.lain.la/api/v0\",\n#     \"https://ipfs.mihir.ch/api/v0\",\n#     \"https://ipfs.telos.miami/api/v0\",\n#     \"https://jorropo.net/api/v0\",\n#     \"https://cf-ipfs.com/api/v0\",\n#     \"https://cloudflare-ipfs.com/api/v0\",\n#     \"https://gateway.ipfs.io/api/v0\",\n#     \"https://infura-ipfs.io/api/v0\",\n#     \"https://via0.com/api/v0\",\n#     \"https://ipfs.azurewebsites.net/api/v0\"\n# ]\n\n# GATEWAYS_API_WRITE = [\n#     \"https://ipfs.io/api/v0\",\n#     \"https://gateway.pinata.cloud/api/v0\",\n#     \"https://cloudflare-ipfs.com/api/v0\",\n#     \"https://dweb.link/api/v0\",\n# ]\n\n\nsource\n\nparse_error_message\n\n parse_error_message (response)\n\nParse error message for raising exceptions\n\n\n\n\nDetails\n\n\n\n\nresponse\nResponse object from requests\n\n\n\n\nsource\n\n\nparse_response\n\n parse_response (response)\n\nParse response object into JSON\n\n\n\n\nDetails\n\n\n\n\nresponse\nResponse object\n\n\n\n\nsource\n\n\nIPFSGateway\n\n IPFSGateway (url, auth=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\ndict_equal\n\n dict_equal (*args)\n\n\nsource\n\n\ndict_hash\n\n dict_hash (dictionary:Dict[str,Any])\n\nMD5 hash of a dictionary.\n\nsource\n\n\ndict_put\n\n dict_put (input_dict, keys, value)\n\ninsert keys that are dot seperated (key1.key2.key3) recursively into a dictionary\n\nsource\n\n\ndict_get\n\n dict_get (input_dict, keys, default_value=False)\n\nget keys that are dot seperated (key1.key2.key3) recursively into a dictionary\n\nsource\n\n\nmake_infura_auth\n\n make_infura_auth (username, password)"
  },
  {
    "objectID": "ipfsspec.tracing.html",
    "href": "ipfsspec.tracing.html",
    "title": "ipfsspec tracing",
    "section": "",
    "text": "source\n\nGatewayTracer\n\n GatewayTracer ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html",
    "href": "tutorial.fastaiipfsspec.html",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "",
    "text": "In this tutorial, we will see how we can use IPFS as the storage for datasets and model in ML workflow."
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#add-a-dataset-to-ipfs-using-local-node",
    "href": "tutorial.fastaiipfsspec.html#add-a-dataset-to-ipfs-using-local-node",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Add a dataset to IPFS using local node",
    "text": "Add a dataset to IPFS using local node\n\nregister_implementation(AsyncIPFSFileSystem.protocol, AsyncIPFSFileSystem)\n\nclass fs:\n    ipfs = fsspec.filesystem(\"ipfs\")\n    file = fsspec.filesystem(\"file\")\n\nChanged to local node\n\n\n\nfs.ipfs.put(path='output/adult_data.csv', rpath='/test_dataset')\n\n'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V'"
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#retrieving-the-dataset-from-ipfs-using-public-node",
    "href": "tutorial.fastaiipfsspec.html#retrieving-the-dataset-from-ipfs-using-public-node",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Retrieving the dataset from IPFS using public node",
    "text": "Retrieving the dataset from IPFS using public node\n\nif fs.file.exists('output/adult_data.csv'):\n    fs.file.rm('output/adult_data.csv', recursive=True)\n    \nprint('Before: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('test/data/dataset/output/*')])\n\nfs.ipfs.get(rpath='QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V', \n            lpath='output/adult_data.csv', # a filename must be given\n            recursive=True, \n            return_cid=False)\n\nprint('After: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/*')])\n\nBefore:  []\nAfter:  ['output/.ipynb_checkpoints', 'output/adult_data.csv', 'output/fol1', 'output/get', 'output/get_file', 'output/get_folder', 'output/test.txt', 'output/test2.txt', 'output/test3.txt']"
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#building-a-tabular-model",
    "href": "tutorial.fastaiipfsspec.html#building-a-tabular-model",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Building a tabular model",
    "text": "Building a tabular model\n\ndf = pd.read_csv('output/adult_data.csv')\ndf.columns = [col.strip() for col in df.columns]\n\n\nsplits = RandomSplitter()(range_of(df))\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\ny_block = CategoryBlock()\n\n\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, y_block=y_block, splits=splits)\n\ndls = to.dataloaders(bs=64)\n\n\nlearn = tabular_learner(dls, [200,100], metrics=accuracy)\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.375601\n      0.359781\n      0.843366\n      00:03\n    \n    \n      1\n      0.359697\n      0.348975\n      0.842905\n      00:02\n    \n    \n      2\n      0.346300\n      0.346158\n      0.844134\n      00:02\n    \n  \n\n\n\n\nlearn.export('output/testmodel.pkl')"
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#adding-modelconfig_files-to-ipfs",
    "href": "tutorial.fastaiipfsspec.html#adding-modelconfig_files-to-ipfs",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Adding model+config_files to IPFS",
    "text": "Adding model+config_files to IPFS\n\nfs.ipfs.put(path='output/testmodel.pkl', rpath='/test_model')\n\n'QmVoD2Bxm7hAZ9BGEg8DeSLstakhfUq1vZouVnwa1zMode'"
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#retrieving-modelconfig_files-from-ipfs",
    "href": "tutorial.fastaiipfsspec.html#retrieving-modelconfig_files-from-ipfs",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Retrieving model+config_files from IPFS",
    "text": "Retrieving model+config_files from IPFS\n\nif fs.file.exists('output/testmodel.pkl'):\n    fs.file.rm('output/testmodel.pkl', recursive=True)\n    \nprint('Before: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/*')])\n\nfs.ipfs.get(rpath='QmSo4beNV5LAr166yZRvy7TNRmCtX4HXyiXqECVvDD6bnt', \n            lpath='output/testmodel.pkl', # a filename must be given\n            recursive=True, \n            return_cid=False)\n\nprint('After: ', [p.lstrip(os.getcwd()) for p in fs.file.glob('output/*')])"
  },
  {
    "objectID": "tutorial.fastaiipfsspec.html#doing-inference-with-retrieved-model",
    "href": "tutorial.fastaiipfsspec.html#doing-inference-with-retrieved-model",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfspec",
    "section": "Doing inference with retrieved model",
    "text": "Doing inference with retrieved model\n\nlearn = load_learner('output/testmodel.pkl')\n\n\ndl = learn.dls.test_dl(df.iloc[:10])\n\n\nlearn.get_preds(dl=dl)\n\n\n\n\n\n\n\n\n(tensor([[0.9134, 0.0866],\n         [0.2455, 0.7545],\n         [0.9745, 0.0255],\n         [0.9120, 0.0880],\n         [0.4120, 0.5880],\n         [0.1225, 0.8775],\n         [0.9719, 0.0281],\n         [0.5242, 0.4758],\n         [0.8494, 0.1506],\n         [0.1704, 0.8296]]),\n tensor([[0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1]], dtype=torch.int8))"
  },
  {
    "objectID": "estuaryapi.html",
    "href": "estuaryapi.html",
    "title": "Estuary",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "estuaryapi.html#standard-ipfs-pinning-api",
    "href": "estuaryapi.html#standard-ipfs-pinning-api",
    "title": "Estuary",
    "section": "Standard IPFS Pinning API",
    "text": "Standard IPFS Pinning API\n\nPinning to/interacting with IPFS through Estuary\n\n\nsource\n\nlist_pins\n\n list_pins (api_key:str)\n\nList all your pins\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\nsource\n\n\nadd_pin\n\n add_pin (api_key:str, file_name:str, cid:str)\n\nAdd a new pin object for the current access token.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nfile_name\nstr\nFile name to pin\n\n\ncid\nstr\nCID to attach\n\n\n\n\nsource\n\n\nget_pin\n\n get_pin (api_key:str, pin_id:str)\n\nGet a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\nsource\n\n\nreplace_pin\n\n replace_pin (api_key:str, pin_id:str)\n\nReplace a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\nsource\n\n\nremove_pin\n\n remove_pin (api_key:str, pin_id:str)\n\nRemove a pinned object by ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npin_id\nstr\nUnique pin ID\n\n\n\n\n\nCollections\n\nsource\n\n\ncreate_coll\n\n create_coll (api_key:str, name:str, description:str)\n\nCreate new collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nname\nstr\nCollection name\n\n\ndescription\nstr\nCollection description\n\n\n\n\nsource\n\n\nadd_content\n\n add_content (api_key:str, collection_id:str, data:list, cids:list)\n\nAdd data to Collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\ndata\nlist\nList of paths to data to be added\n\n\ncids\nlist\nList of respective CIDs\n\n\n\n\nsource\n\n\nlist_colls\n\n list_colls (api_key:str)\n\nList your collections\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\nsource\n\n\nlist_coll_content\n\n list_coll_content (api_key:str, collection_id:str)\n\nList contents of a collection from ID\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\n\n\nsource\n\n\nlist_content_path\n\n list_content_path (api_key:str, collection_id:str, path:str)\n\nList content of a path in collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\npath\nstr\nPath in collection to list files from\n\n\n\n\nsource\n\n\nadd_content_path\n\n add_content_path (api_key:str, collection_id:str, path:str)\n\nAdd content to a specific file system path in an IPFS collection\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncollection_id\nstr\nCollection ID\n\n\npath\nstr\nPath in collection to add files to\n\n\n\n\n\nEstuary base API\n\nsource\n\n\nadd_key\n\n add_key (api_key, expiry='24h')\n\nAdd client safe upload key\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\napi_key\n\n\nYour Estuary API key\n\n\nexpiry\nstr\n24h\nExpiry of upload key\n\n\n\n\nsource\n\n\nadd_data\n\n add_data (api_key:str, path_to_file:str)\n\nUpload file to Estuary\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npath_to_file\nstr\nPath to file you want to upload\n\n\n\n\nsource\n\n\nadd_cid\n\n add_cid (api_key:str, file_name:str, cid:str)\n\nUse an existing IPFS CID to make storage deals.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\nfile_name\nstr\nFile name to add to CID\n\n\ncid\nstr\nCID for file\n\n\n\n\nsource\n\n\nadd_car\n\n add_car (api_key:str, path_to_file:str)\n\nWrite a Content-Addressable Archive (CAR) file, and make storage deals for its contents.\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\npath_to_file\nstr\nPath to file to store\n\n\n\n\nsource\n\n\nmake_deal\n\n make_deal (api_key:str, content_id:str, provider_id:str)\n\nMake a deal with a storage provider and a file you have already uploaded to Estuary\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncontent_id\nstr\nContent ID on Estuary\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\nsource\n\n\nview_data_cid\n\n view_data_cid (api_key:str, cid:str)\n\nView CID information\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ncid\nstr\nCID\n\n\n\n\nsource\n\n\nlist_data\n\n list_data (api_key:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\nsource\n\n\nlist_deals\n\n list_deals (api_key:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\n\n\nsource\n\n\nget_deal_status\n\n get_deal_status (api_key:str, deal_id:str)\n\nGet deal status by id\n\n\n\n\nType\nDetails\n\n\n\n\napi_key\nstr\nYour Estuary API key\n\n\ndeal_id\nstr\nDeal ID\n\n\n\n\nsource\n\n\nget_node_stats\n\n get_node_stats ()\n\nGet Estuary node stats\n\nsource\n\n\nget_deal_data\n\n get_deal_data ()\n\nGet on-chain deal data\n\nsource\n\n\nget_miner_ask\n\n get_miner_ask (miner_id:str)\n\nGet the query ask and verified ask for any miner\n\n\n\n\nType\nDetails\n\n\n\n\nminer_id\nstr\nMiner ID\n\n\n\n\nsource\n\n\nget_failure_logs\n\n get_failure_logs (miner_id:str)\n\nGet all of the failure logs for a specific miner\n\n\n\n\nType\nDetails\n\n\n\n\nminer_id\nstr\nMiner ID\n\n\n\n\nsource\n\n\nget_deal_logs\n\n get_deal_logs (provider_id:str)\n\nGet deal logs by provider\n\n\n\n\nType\nDetails\n\n\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\nsource\n\n\nget_provider_stats\n\n get_provider_stats (provider_id:str)\n\nGet provider stats\n\n\n\n\nType\nDetails\n\n\n\n\nprovider_id\nstr\nProvider ID\n\n\n\n\nsource\n\n\nlist_providers\n\n list_providers ()\n\nList Estuary providers\n\n\nDownloading data\n\nsource\n\n\nget_data\n\n get_data (cid:str, path_name:str)\n\nDownload data from Estuary CID\n\n\n\n\nType\nDetails\n\n\n\n\ncid\nstr\nData CID\n\n\npath_name\nstr\nPath and filename to store the file at"
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html",
    "href": "tutorial.fastaiandipfspy.html",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "",
    "text": "In this tutorial, we will see how we can use IPFS as the storage for datasets and model in ML workflow."
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html#add-a-dataset-to-ipfs-using-infura-node",
    "href": "tutorial.fastaiandipfspy.html#add-a-dataset-to-ipfs-using-infura-node",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "Add a dataset to IPFS using infura node",
    "text": "Add a dataset to IPFS using infura node\n\napi = IPFSApi()\n\nChanged to local node\n\n\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nres, obj = api.add_items('output/adult_data.csv'); obj\n\n[{'Name': 'adult_data.csv', 'Bytes': 262144},\n {'Name': 'adult_data.csv', 'Bytes': 524288},\n {'Name': 'adult_data.csv', 'Bytes': 786432},\n {'Name': 'adult_data.csv', 'Bytes': 1048576},\n {'Name': 'adult_data.csv', 'Bytes': 1310720},\n {'Name': 'adult_data.csv', 'Bytes': 1572864},\n {'Name': 'adult_data.csv', 'Bytes': 1835008},\n {'Name': 'adult_data.csv', 'Bytes': 2097152},\n {'Name': 'adult_data.csv', 'Bytes': 2359296},\n {'Name': 'adult_data.csv', 'Bytes': 2621440},\n {'Name': 'adult_data.csv', 'Bytes': 2883584},\n {'Name': 'adult_data.csv', 'Bytes': 3145728},\n {'Name': 'adult_data.csv', 'Bytes': 3407872},\n {'Name': 'adult_data.csv', 'Bytes': 3670016},\n {'Name': 'adult_data.csv', 'Bytes': 3932160},\n {'Name': 'adult_data.csv', 'Bytes': 3974475},\n {'Name': 'adult_data.csv',\n  'Hash': 'QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V',\n  'Size': '3975476'}]"
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html#retrieving-a-dataset-from-ipfs-using-public-node",
    "href": "tutorial.fastaiandipfspy.html#retrieving-a-dataset-from-ipfs-using-public-node",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "Retrieving a dataset from IPFS using public node",
    "text": "Retrieving a dataset from IPFS using public node\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nres, obj = api.cat_items('QmZnxARhJWsCbTxiAzoRhnxHgMtoEkNJNS8DGLCBEMvm4V')\n\n\nwith open('output/adult_data_dl.csv', 'wb') as f:\n    f.write(res.content)\n\n\ndf = pd.read_csv('output/adult_data_dl.csv')\n\n\ndf.columns = [col.strip() for col in df.columns]"
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html#building-a-tabular-model",
    "href": "tutorial.fastaiandipfspy.html#building-a-tabular-model",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "Building a tabular model",
    "text": "Building a tabular model\n\nsplits = RandomSplitter()(range_of(df))\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\ny_block = CategoryBlock()\n\n\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, y_block=y_block, splits=splits)\n\ndls = to.dataloaders(bs=64)\n\n\nlearn = tabular_learner(dls, [200,100], metrics=accuracy)\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.376169\n      0.351168\n      0.838913\n      00:02\n    \n    \n      1\n      0.368451\n      0.349306\n      0.836456\n      00:02\n    \n    \n      2\n      0.357292\n      0.343430\n      0.840756\n      00:02\n    \n  \n\n\n\n\nlearn.export('output/testmodel.pkl')"
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html#adding-modelconfig_files-to-ipfs",
    "href": "tutorial.fastaiandipfspy.html#adding-modelconfig_files-to-ipfs",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "Adding model+config_files to IPFS",
    "text": "Adding model+config_files to IPFS\n\napi.change_gateway_type = 'infura'\n\nChanged to infura node\n\n\n\nres, obj = api.add_items('output/testmodel.pkl'); obj\n\n[{'Name': 'testmodel.pkl', 'Bytes': 242379},\n {'Name': 'testmodel.pkl',\n  'Hash': 'QmR77qXp7CYEg6kHA3z77mcayTmm9hoXz7YQHFz9WjphiE',\n  'Size': '242393'}]"
  },
  {
    "objectID": "tutorial.fastaiandipfspy.html#retrieving-modelconfig_files-from-ipfs",
    "href": "tutorial.fastaiandipfspy.html#retrieving-modelconfig_files-from-ipfs",
    "title": "Tutorial - IPFS + ML using ipfspy.ipfshttpapi",
    "section": "Retrieving model+config_files from IPFS",
    "text": "Retrieving model+config_files from IPFS\n\napi.change_gateway_type = 'public'\n\nChanged to public node\n\n\n\nres, obj = api.cat_items('QmR77qXp7CYEg6kHA3z77mcayTmm9hoXz7YQHFz9WjphiE')\n\n\nwith open('output/testmodel_dl.pkl', 'wb') as f:\n    f.write(res.content)\n\n\nlearn = load_learner('output/testmodel_dl.pkl')\n\n\ndl = learn.dls.test_dl(df.iloc[:10])\n\n\nlearn.get_preds(dl=dl)\n\n\n\n\n\n\n\n\n(tensor([[0.9282, 0.0718],\n         [0.3556, 0.6444],\n         [0.9667, 0.0333],\n         [0.8731, 0.1269],\n         [0.4372, 0.5628],\n         [0.1317, 0.8683],\n         [0.9819, 0.0181],\n         [0.5074, 0.4926],\n         [0.8733, 0.1267],\n         [0.1803, 0.8197]]),\n tensor([[0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [0],\n         [1],\n         [1],\n         [1]], dtype=torch.int8))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ipfspy",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "ipfspy",
    "section": "Install",
    "text": "Install\npip install ipfspy"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "ipfspy",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "ipfsspec.bufferedfile.html",
    "href": "ipfsspec.bufferedfile.html",
    "title": "ipfsspec buffered file",
    "section": "",
    "text": "source\n\nIPFSBufferedFile\n\n IPFSBufferedFile (fs, path, mode='rb', block_size='default',\n                   autocommit=True, cache_type='readahead',\n                   cache_options=None, size=None, pin=False, rpath=None,\n                   **kwargs)\n\nConvenient class to derive from to provide buffering\nIn the case that the backend does not provide a pythonic file-like object already, this class contains much of the logic to build one. The only methods that need to be overridden are _upload_chunk, _initiate_upload and _fetch_range.\n\nsource\n\n\nLocalFileOpener\n\n LocalFileOpener (path, mode, autocommit=True, fs=None, compression=None,\n                  **kwargs)\n\nThe abstract base class for all I/O classes.\nThis class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked.\nEven though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called.\nThe basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data.\nNote that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError.\nIOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream.\nIOBase also supports the :keyword:with statement. In this example, fp is closed after the suite of the with statement is complete:\nwith open(‘spam.txt’, ‘r’) as fp: fp.write(‘Spam and eggs!’)"
  }
]